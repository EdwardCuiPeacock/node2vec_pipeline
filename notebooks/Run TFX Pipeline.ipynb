{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-independence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:57.549189Z",
     "start_time": "2021-04-06T17:35:57.525460Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/node2vec_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/node2vec_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-investigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:57.786854Z",
     "start_time": "2021-04-06T17:35:57.782102Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = \"res-nbcupea-dev-ds-sandbox-001\"\n",
    "ENDPOINT='https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com' # Enter your ENDPOINT here.\n",
    "CUSTOM_TFX_IMAGE = CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-facial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:58.084336Z",
     "start_time": "2021-04-06T17:35:58.077871Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory and a directory containing `skaffold`, as well as `tfx`\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "accepted-celebration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T15:48:56.418252Z",
     "start_time": "2021-04-08T15:48:30.070264Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 15:48:30.501681: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-08 15:48:30.501804: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline -> gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  72.66MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.26.0\n",
      "[Skaffold]  ---> 3230796f5988\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> ddad9cd6ca10\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> a4add0d2f0e2\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in fdec81b2a100\n",
      "[Skaffold] Removing intermediate container fdec81b2a100\n",
      "[Skaffold]  ---> 2cce97f3d0c6\n",
      "[Skaffold] Successfully built 2cce97f3d0c6\n",
      "[Skaffold] Successfully tagged gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]\n",
      "[Skaffold] a396df9fb94c: Preparing\n",
      "[Skaffold] f59694ec6d1f: Preparing\n",
      "[Skaffold] 23b988d0a368: Preparing\n",
      "[Skaffold] a0fdd9e79e13: Preparing\n",
      "[Skaffold] 7ed3e18de502: Preparing\n",
      "[Skaffold] fd335a0d8c48: Preparing\n",
      "[Skaffold] 7d985a940d09: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] fd335a0d8c48: Waiting\n",
      "[Skaffold] 7d985a940d09: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 7ed3e18de502: Layer already exists\n",
      "[Skaffold] f59694ec6d1f: Layer already exists\n",
      "[Skaffold] 23b988d0a368: Layer already exists\n",
      "[Skaffold] a0fdd9e79e13: Layer already exists\n",
      "[Skaffold] fd335a0d8c48: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 7d985a940d09: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] a396df9fb94c: Pushed\n",
      "[Skaffold] latest: digest: sha256:4ea79cbcf9bdde5f57842b4ea71168f1569613030bfa68e7b85d90f662a5e543 size: 3482\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-04-08 15:48:51.308883: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-08 15:48:51.308949: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:Current tfx image used: gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline@sha256:4ea79cbcf9bdde5f57842b4ea71168f1569613030bfa68e7b85d90f662a5e543\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:trainer arguments\n",
      "INFO:absl:{'run_fn': 'models.node2vec.model.run_fn', 'transformed_examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "), 'schema': Channel(\n",
      "    type_name: Schema\n",
      "    artifacts: []\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "), 'train_args': splits: \"train\"\n",
      "num_steps: 100\n",
      ", 'eval_args': splits: \"train\"\n",
      "num_steps: 50\n",
      ", 'custom_config': {'model_config': {'query_script_path': 'data/graph_data.sql', 'query_debug_settings': '', 'num_epochs': 30, 'train_batch_size': 128, 'eval_batch_size': None, 'embed_size': 32, 'num_neg_samples': 15, 'walk_length': 30, 'train_repetitions': 6, 'eval_repetitions': 1, 'p': 1.0, 'q': 1.0, 'window_size': 8}, 'system_config': {'GCS_BUCKET_NAME': 'gs://edc-dev/kubeflowpipelines-default', 'GOOGLE_CLOUD_REGION': 'us-east1', 'GOOGLE_CLOUD_PROJECT': 'res-nbcupea-dev-ds-sandbox-001', 'ENDPOINT': 'df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com', 'TFX_IMAGE': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'PIPELINE_ROOT': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0', 'MODEL_SERVE_DIR': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0/serving_model', 'DATAFLOW_BEAM_PIPELINE_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'runner': 'DataflowRunner', 'temp_location': 'gs://edc-dev/kubeflowpipelines-default/tmp', 'region': 'us-east1', 'disk_size_gb': 50, 'machine_type': 'e2-standard-8'}, 'GCP_AI_PLATFORM_TRAINING_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}, 'KUBEFLOW_RUNNER': 'kubeflow_runner.py', 'enable_cache': False, 'enable_gpc_ai_platform_training': True, 'preprocessing_fn': 'models.preprocessing.preprocessing_fn', 'run_fn': 'models.node2vec.model.run_fn'}, 'ai_platform_training_args': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}, 'ai_platform_training_job_id': 'tfx_node2vec_sports_syn_0_1_0_20210408154854'}, 'custom_executor_spec': <tfx.dsl.components.base.executor_spec.ExecutorClassSpec object at 0x7f37cb14b950>}\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/node2vec_pipeline/node2vec_sports_syn_0_1_0.tar.gz\n",
      "{'created_at': datetime.datetime(2021, 4, 8, 15, 48, 55, tzinfo=tzlocal()),\n",
      " 'default_version': {'code_source_url': None,\n",
      "                     'created_at': datetime.datetime(2021, 4, 8, 15, 48, 55, tzinfo=tzlocal()),\n",
      "                     'id': '726d592c-5035-43e9-8d3d-66c0ead84643',\n",
      "                     'name': 'node2vec_sports_syn_0_1_0',\n",
      "                     'package_url': None,\n",
      "                     'parameters': [{'name': 'pipeline-root',\n",
      "                                     'value': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0'}],\n",
      "                     'resource_references': [{'key': {'id': '726d592c-5035-43e9-8d3d-66c0ead84643',\n",
      "                                                      'type': 'PIPELINE'},\n",
      "                                              'name': None,\n",
      "                                              'relationship': 'OWNER'}]},\n",
      " 'description': None,\n",
      " 'error': None,\n",
      " 'id': '726d592c-5035-43e9-8d3d-66c0ead84643',\n",
      " 'name': 'node2vec_sports_syn_0_1_0',\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0'}],\n",
      " 'url': None}\n",
      "Please access the pipeline detail page at https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/pipelines/details/726d592c-5035-43e9-8d3d-66c0ead84643\n",
      "Pipeline \"node2vec_sports_syn_0_1_0\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initial build: need to rebuilt \n",
    "# every time there is a version change\n",
    "!tfx pipeline create  \\\n",
    "--pipeline-path=/home/jupyter/node2vec_pipeline/kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "failing-alloy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T15:49:16.526046Z",
     "start_time": "2021-04-08T15:49:11.624237Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 15:49:12.051453: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-08 15:49:12.051519: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: node2vec_sports_syn_0_1_0\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: node2vec_sports_syn_0_1_0\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name             | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===========================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| node2vec_sports_syn_0_1_0 | 8ddb58c7-c1c5-42ff-abc2-74c19b0727f0 |          | 2021-04-08T15:49:15+00:00 | https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/8ddb58c7-c1c5-42ff-abc2-74c19b0727f0 |\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = \"node2vec_sports_syn_0_1_0\"\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "employed-poison",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T16:41:42.040658Z",
     "start_time": "2021-04-09T16:41:04.564727Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-09 16:41:05.068973: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-09 16:41:05.069038: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline -> gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  78.24MB\n",
      "[Skaffold] Step 1/5 : FROM tensorflow/tfx:0.26.0\n",
      "[Skaffold]  ---> 3230796f5988\n",
      "[Skaffold] Step 2/5 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> ddad9cd6ca10\n",
      "[Skaffold] Step 3/5 : COPY ./ ./\n",
      "[Skaffold]  ---> a57a3d31861d\n",
      "[Skaffold] Step 4/5 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in af79b09c5913\n",
      "[Skaffold] Removing intermediate container af79b09c5913\n",
      "[Skaffold]  ---> 561b969eb14d\n",
      "[Skaffold] Step 5/5 : RUN pip install --no-cache-dir psutil\n",
      "[Skaffold]  ---> Running in 00c066e852fb\n",
      "[Skaffold] Collecting psutil\n",
      "[Skaffold]   Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
      "[Skaffold] Installing collected packages: psutil\n",
      "[Skaffold] Successfully installed psutil-5.8.0\n",
      "[Skaffold] \u001b[91mWARNING: You are using pip version 20.2; however, version 21.0.1 is available.\n",
      "[Skaffold] You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n",
      "[Skaffold] \u001b[0mRemoving intermediate container 00c066e852fb\n",
      "[Skaffold]  ---> 347e0e7cb6fe\n",
      "[Skaffold] Successfully built 347e0e7cb6fe\n",
      "[Skaffold] Successfully tagged gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]\n",
      "[Skaffold] 65be147be197: Preparing\n",
      "[Skaffold] 9a6f99b319d1: Preparing\n",
      "[Skaffold] f59694ec6d1f: Preparing\n",
      "[Skaffold] 23b988d0a368: Preparing\n",
      "[Skaffold] a0fdd9e79e13: Preparing\n",
      "[Skaffold] 7ed3e18de502: Preparing\n",
      "[Skaffold] fd335a0d8c48: Preparing\n",
      "[Skaffold] 7d985a940d09: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] fd335a0d8c48: Waiting\n",
      "[Skaffold] 7d985a940d09: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] 7ed3e18de502: Waiting\n",
      "[Skaffold] 23b988d0a368: Layer already exists\n",
      "[Skaffold] f59694ec6d1f: Layer already exists\n",
      "[Skaffold] a0fdd9e79e13: Layer already exists\n",
      "[Skaffold] 7ed3e18de502: Layer already exists\n",
      "[Skaffold] fd335a0d8c48: Layer already exists\n",
      "[Skaffold] 7d985a940d09: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 65be147be197: Pushed\n",
      "[Skaffold] 9a6f99b319d1: Pushed\n",
      "[Skaffold] latest: digest: sha256:9f62f5fd8a29c2c2aa770d06e052ddb5798c181b7714030e7827027cae445ea5 size: 3692\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-04-09 16:41:31.363393: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-09 16:41:31.363465: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:Current tfx image used: gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline@sha256:9f62f5fd8a29c2c2aa770d06e052ddb5798c181b7714030e7827027cae445ea5\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:trainer arguments\n",
      "INFO:absl:{'run_fn': 'models.node2vec.model.run_fn', 'transformed_examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "), 'schema': Channel(\n",
      "    type_name: Schema\n",
      "    artifacts: []\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "), 'train_args': splits: \"train\"\n",
      "num_steps: 100\n",
      ", 'eval_args': splits: \"train\"\n",
      "num_steps: 50\n",
      ", 'custom_config': {'model_config': {'query_script_path': 'data/graph_data.sql', 'query_debug_settings': '', 'seed': 42, 'num_epochs': 30, 'train_batch_size': 128, 'eval_batch_size': None, 'embed_size': 32, 'num_neg_samples': 15, 'walk_length': 30, 'train_repetitions': 6, 'eval_repetitions': 1, 'p': 1.0, 'q': 1.0, 'window_size': 8}, 'system_config': {'GCS_BUCKET_NAME': 'gs://edc-dev/kubeflowpipelines-default', 'GOOGLE_CLOUD_REGION': 'us-east1', 'GOOGLE_CLOUD_PROJECT': 'res-nbcupea-dev-ds-sandbox-001', 'ENDPOINT': 'df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com', 'TFX_IMAGE': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'PIPELINE_ROOT': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0', 'MODEL_SERVE_DIR': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0/serving_model', 'DATAFLOW_BEAM_PIPELINE_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'runner': 'DataflowRunner', 'temp_location': 'gs://edc-dev/kubeflowpipelines-default/tmp', 'region': 'us-east1', 'disk_size_gb': 50, 'machine_type': 'e2-standard-8'}, 'GCP_AI_PLATFORM_TRAINING_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}, 'KUBEFLOW_RUNNER': 'kubeflow_runner.py', 'enable_cache': True, 'enable_gpc_ai_platform_training': True, 'preprocessing_fn': 'models.preprocessing.preprocessing_fn', 'run_fn': 'models.node2vec.model.run_fn'}, 'ai_platform_training_args': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}, 'ai_platform_training_job_id': 'tfx_node2vec_sports_syn_0_1_0_20210409164134'}, 'custom_executor_spec': <tfx.dsl.components.base.executor_spec.ExecutorClassSpec object at 0x7f37839c0b10>}\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/node2vec_pipeline/node2vec_sports_syn_0_1_0.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 4, 9, 16, 41, 35, tzinfo=tzlocal()),\n",
      " 'id': '23a9e995-b3fb-4178-92f6-d6e2ad4871a5',\n",
      " 'name': 'node2vec_sports_syn_0_1_0_20210409164135',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_1_0'}],\n",
      " 'resource_references': [{'key': {'id': '726d592c-5035-43e9-8d3d-66c0ead84643',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/pipelines/details/726d592c-5035-43e9-8d3d-66c0ead84643\n",
      "Pipeline \"node2vec_sports_syn_0_1_0\" updated successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-09 16:41:37.126767: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-09 16:41:37.126819: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: node2vec_sports_syn_0_1_0\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: node2vec_sports_syn_0_1_0\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name             | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===========================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| node2vec_sports_syn_0_1_0 | bfc22f93-954c-4257-a8f8-25fdfa9a5049 |          | 2021-04-09T16:41:40+00:00 | https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/bfc22f93-954c-4257-a8f8-25fdfa9a5049 |\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=/home/jupyter/node2vec_pipeline/kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "\n",
    "PIPELINE_NAME = \"node2vec_sports_syn_0_1_0\"\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bottom-camping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:29:47.845171Z",
     "start_time": "2021-04-09T14:29:47.776363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048000112"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sys.getsizeof(np.random.randint(0, 1000000, size=(int(2E6), 3)).astype(np.int64)) / 1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "supposed-onion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:32:54.527561Z",
     "start_time": "2021-04-09T14:32:54.327453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = np.random.randint(0, 100000, size=int(2E6)).astype(np.int64)\n",
    "cols = np.random.randint(0, 100000, size=int(2E6)).astype(np.int64)\n",
    "data = np.random.randn(int(2E6)).astype(np.float32)\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "sparse_mat = coo_matrix(( data, (rows, cols)), shape=(150000, 150000))\n",
    "sys.getsizeof(sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "vocational-answer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:33:19.970322Z",
     "start_time": "2021-04-09T14:33:19.965098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.040000288"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sys.getsizeof(rows) + sys.getsizeof(cols) + sys.getsizeof(data)) / 1E9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
