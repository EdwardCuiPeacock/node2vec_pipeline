{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-independence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:57.549189Z",
     "start_time": "2021-04-06T17:35:57.525460Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/node2vec_pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jupyter/node2vec_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "destroyed-investigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:57.786854Z",
     "start_time": "2021-04-06T17:35:57.782102Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = \"res-nbcupea-dev-ds-sandbox-001\"\n",
    "ENDPOINT='https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com' # Enter your ENDPOINT here.\n",
    "CUSTOM_TFX_IMAGE = CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-facial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-06T17:35:58.084336Z",
     "start_time": "2021-04-06T17:35:58.077871Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "# Set `PATH` to include user python binary directory and a directory containing `skaffold`, as well as `tfx`\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "accepted-celebration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T15:25:36.799951Z",
     "start_time": "2021-04-01T15:25:08.491366Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02 15:36:07.983226: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-02 15:36:07.983285: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline -> gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  72.19MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.26.0\n",
      "[Skaffold]  ---> 3230796f5988\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> ddad9cd6ca10\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 94ac2867a5ce\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in eba2657587ee\n",
      "[Skaffold] Removing intermediate container eba2657587ee\n",
      "[Skaffold]  ---> d862fe1f2573\n",
      "[Skaffold] Successfully built d862fe1f2573\n",
      "[Skaffold] Successfully tagged gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]\n",
      "[Skaffold] 13077f58d105: Preparing\n",
      "[Skaffold] f59694ec6d1f: Preparing\n",
      "[Skaffold] 23b988d0a368: Preparing\n",
      "[Skaffold] a0fdd9e79e13: Preparing\n",
      "[Skaffold] 7ed3e18de502: Preparing\n",
      "[Skaffold] fd335a0d8c48: Preparing\n",
      "[Skaffold] 7d985a940d09: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] fd335a0d8c48: Waiting\n",
      "[Skaffold] 7d985a940d09: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 23b988d0a368: Layer already exists\n",
      "[Skaffold] 7ed3e18de502: Layer already exists\n",
      "[Skaffold] f59694ec6d1f: Layer already exists\n",
      "[Skaffold] a0fdd9e79e13: Layer already exists\n",
      "[Skaffold] fd335a0d8c48: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 7d985a940d09: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 13077f58d105: Pushed\n",
      "[Skaffold] latest: digest: sha256:e6c0a6322ecc63489430f82fa029332c36b9f44b4b461f2baf160614f9a3fd9e size: 3482\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-04-02 15:36:30.863073: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-02 15:36:30.863132: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:Current tfx image used: gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline@sha256:e6c0a6322ecc63489430f82fa029332c36b9f44b4b461f2baf160614f9a3fd9e\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:trainer arguments\n",
      "INFO:absl:{'run_fn': 'models.node2vec.model.run_fn', 'transformed_examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "), 'schema': Channel(\n",
      "    type_name: Schema\n",
      "    artifacts: []\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "), 'train_args': num_steps: 100\n",
      ", 'eval_args': num_steps: 50\n",
      ", 'custom_config': {'model_config': {'query_script_path': 'data/graph_data.sql', 'num_epochs': 1000, 'train_batch_size': 150, 'eval_batch_size': None, 'eval_accuracy_threshold': 0.6, 'embed_size': 32, 'num_neg_samples': 12, 'walk_length': 80, 'train_repetitions': 5, 'eval_repetitions': 1, 'p': 0.2, 'q': 0.8}, 'system_config': {'GCS_BUCKET_NAME': 'gs://edc-dev/kubeflowpipelines-default', 'GOOGLE_CLOUD_REGION': 'us-east1', 'GOOGLE_CLOUD_PROJECT': 'res-nbcupea-dev-ds-sandbox-001', 'ENDPOINT': 'df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com', 'TFX_IMAGE': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'PIPELINE_ROOT': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_0_0', 'MODEL_SERVE_DIR': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_0_0/serving_model', 'DATAFLOW_BEAM_PIPELINE_ARGS': ['--project=res-nbcupea-dev-ds-sandbox-001', '--runner=DataflowRunner', '--temp_location=gs://edc-dev/kubeflowpipelines-default/tmp', '--region=us-east1', '--disk_size_gb=50', '--machine_type=e2-standard-8'], 'GCP_AI_PLATFORM_TRAINING_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east1', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline'}}, 'KUBEFLOW_RUNNER': 'kubeflow_runner.py', 'enable_cache': False, 'preprocessing_fn': 'models.preprocessing.preprocessing_fn', 'run_fn': 'models.node2vec.model.run_fn'}}, 'custom_executor_spec': <tfx.dsl.components.base.executor_spec.ExecutorClassSpec object at 0x7f45983c6690>}\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/node2vec_pipeline/node2vec_sports_syn_0_0_0.tar.gz\n",
      "Pipeline \"node2vec_sports_syn_0_0_0\" already exists.\n"
     ]
    }
   ],
   "source": [
    "# Initial build\n",
    "!tfx pipeline create  \\\n",
    "--pipeline-path=/home/jupyter/node2vec_pipeline/kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "employed-poison",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T17:42:33.677377Z",
     "start_time": "2021-04-07T17:41:59.388483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07 17:41:59.875123: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-07 17:41:59.875179: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline -> gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  72.35MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.26.0\n",
      "[Skaffold]  ---> 3230796f5988\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> ddad9cd6ca10\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 133e2f020275\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 2a4612e767e3\n",
      "[Skaffold] Removing intermediate container 2a4612e767e3\n",
      "[Skaffold]  ---> ccab7251c13b\n",
      "[Skaffold] Successfully built ccab7251c13b\n",
      "[Skaffold] Successfully tagged gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline]\n",
      "[Skaffold] 31faa7ad8be3: Preparing\n",
      "[Skaffold] f59694ec6d1f: Preparing\n",
      "[Skaffold] 23b988d0a368: Preparing\n",
      "[Skaffold] a0fdd9e79e13: Preparing\n",
      "[Skaffold] 7ed3e18de502: Preparing\n",
      "[Skaffold] fd335a0d8c48: Preparing\n",
      "[Skaffold] 7d985a940d09: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] fd335a0d8c48: Waiting\n",
      "[Skaffold] 7d985a940d09: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] a0fdd9e79e13: Layer already exists\n",
      "[Skaffold] 23b988d0a368: Layer already exists\n",
      "[Skaffold] 7ed3e18de502: Layer already exists\n",
      "[Skaffold] f59694ec6d1f: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 7d985a940d09: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] fd335a0d8c48: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 31faa7ad8be3: Pushed\n",
      "[Skaffold] latest: digest: sha256:361d5613e2b49b604ddf46366aef3a3a105242c87cf90e5932cddc39d7f1725e size: 3482\n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-04-07 17:42:23.539142: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-07 17:42:23.539228: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:Current tfx image used: gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline@sha256:361d5613e2b49b604ddf46366aef3a3a105242c87cf90e5932cddc39d7f1725e\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:trainer arguments\n",
      "INFO:absl:{'run_fn': 'models.node2vec.model.run_fn', 'transformed_examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "), 'schema': Channel(\n",
      "    type_name: Schema\n",
      "    artifacts: []\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "), 'train_args': splits: \"train\"\n",
      "num_steps: 100\n",
      ", 'eval_args': splits: \"train\"\n",
      "num_steps: 50\n",
      ", 'custom_config': {'model_config': {'query_script_path': 'data/graph_data.sql', 'num_epochs': 10, 'train_batch_size': 128, 'eval_batch_size': None, 'eval_accuracy_threshold': 0.6, 'embed_size': 32, 'num_neg_samples': 17, 'walk_length': 3, 'train_repetitions': 5, 'eval_repetitions': 1, 'p': 0.2, 'q': 0.8, 'window_size': 2}, 'system_config': {'GCS_BUCKET_NAME': 'gs://edc-dev/kubeflowpipelines-default', 'GOOGLE_CLOUD_REGION': 'us-east1', 'GOOGLE_CLOUD_PROJECT': 'res-nbcupea-dev-ds-sandbox-001', 'ENDPOINT': 'df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com', 'TFX_IMAGE': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'PIPELINE_ROOT': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_0_0', 'MODEL_SERVE_DIR': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_0_0/serving_model', 'DATAFLOW_BEAM_PIPELINE_ARGS': ['--project=res-nbcupea-dev-ds-sandbox-001', '--runner=DataflowRunner', '--temp_location=gs://edc-dev/kubeflowpipelines-default/tmp', '--region=us-east1', '--disk_size_gb=50', '--machine_type=e2-standard-8'], 'GCP_AI_PLATFORM_TRAINING_ARGS': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}, 'KUBEFLOW_RUNNER': 'kubeflow_runner.py', 'enable_cache': True, 'enable_gpc_ai_platform_training': True, 'preprocessing_fn': 'models.preprocessing.preprocessing_fn', 'run_fn': 'models.node2vec.model.run_fn'}, 'ai_platform_training_args': {'project': 'res-nbcupea-dev-ds-sandbox-001', 'region': 'us-east4', 'scaleTier': 'CUSTOM', 'masterType': 'n1-highmem-16', 'masterConfig': {'imageUri': 'gcr.io/res-nbcupea-dev-ds-sandbox-001/tfx-pipeline', 'acceleratorConfig': {'count': 1, 'type': 'NVIDIA_TESLA_P4'}}}}, 'custom_executor_spec': <tfx.dsl.components.base.executor_spec.ExecutorClassSpec object at 0x7f64cb810850>}\n",
      "WARNING:absl:`instance_name` is deprecated, please set the node id directly using `with_id()` or the `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/node2vec_pipeline/node2vec_sports_syn_0_0_0.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 4, 7, 17, 42, 27, tzinfo=tzlocal()),\n",
      " 'id': '74cf3d65-649d-4e65-811b-a5fa7b8e7f9c',\n",
      " 'name': 'node2vec_sports_syn_0_0_0_20210407174227',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://edc-dev/kubeflowpipelines-default/tfx_pipeline_output/node2vec_sports_syn_0_0_0'}],\n",
      " 'resource_references': [{'key': {'id': 'e8eca911-c543-4657-9889-e297113c4d74',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/pipelines/details/e8eca911-c543-4657-9889-e297113c4d74\n",
      "Pipeline \"node2vec_sports_syn_0_0_0\" updated successfully.\n",
      "2021-04-07 17:42:28.957071: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-07 17:42:28.957121: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Creating a run for pipeline: node2vec_sports_syn_0_0_0\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: node2vec_sports_syn_0_0_0\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name             | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===========================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| node2vec_sports_syn_0_0_0 | 3f106da0-73b3-4913-9ce7-7f484eea47d4 |          | 2021-04-07T17:42:32+00:00 | https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/3f106da0-73b3-4913-9ce7-7f484eea47d4 |\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=/home/jupyter/node2vec_pipeline/kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "\n",
    "PIPELINE_NAME = \"node2vec_sports_syn_0_0_0\"\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "failing-alloy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T19:08:28.350161Z",
     "start_time": "2021-04-02T19:08:23.190221Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02 19:08:23.643879: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2021-04-02 19:08:23.643935: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: node2vec_sports_syn_0_0_0\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: node2vec_sports_syn_0_0_0\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name             | run_id                               | status   | created_at                | link                                                                                                                     |\n",
      "+===========================+======================================+==========+===========================+==========================================================================================================================+\n",
      "| node2vec_sports_syn_0_0_0 | 976a8304-32dd-4105-87ff-438a344b40c4 |          | 2021-04-02T19:08:27+00:00 | https://df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com/#/runs/details/976a8304-32dd-4105-87ff-438a344b40c4 |\n",
      "+---------------------------+--------------------------------------+----------+---------------------------+--------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = \"node2vec_sports_syn_0_0_0\"\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-romance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
