# This file sets up the environment variables of the job run
pipeline_name: node2vec_sports_syn
pipeline_version: "0_1_0" # must be in  this format
description: |
  This pipeline runs the node2vec model on TFX with tensorflow / Keras
  system_configuration: All fields are required for the pipeline to run properly
  model_configuration: Additional custom arguments exposed to the model.
  For strings, array of strings,or object (dict) fields, Jinja2 templating is supported.
system_configurations:
  GCS_BUCKET_NAME: 
    description: Google Cloud Storage bucket name to store pipeline outputs
    type: string
    value: "gs://edc-dev/kubeflowpipelines-default"
  GOOGLE_CLOUD_REGION:
    description: Region to use GCP services including BigQuery, Dataflow and Cloud AI Platform.
    type: string
    value: us-east1
  GOOGLE_CLOUD_PROJECT:
    description: Google cloud project name
    type: string
    value: res-nbcupea-dev-ds-sandbox-001
  ENDPOINT:
    description: Google Cloud Endpoint
    type: string
    value: df6bc4688870067-dot-us-east1.pipelines.googleusercontent.com
  TFX_IMAGE:
    description: |
      Name of the repository on Container Registry.
    type: string
    value: "gcr.io/{{ GOOGLE_CLOUD_PROJECT }}/tfx-pipeline"
  PIPELINE_ROOT:
    description: |
      TFX produces two types of outputs, files and metadata.
      Files will be created under PIPELINE_ROOT directory.
    type: string
    value: "{{ GCS_BUCKET_NAME }}/tfx_pipeline_output/{{ pipeline_name }}_{{ pipeline_version }}"
  MODEL_SERVE_DIR:
    description: |
      The last component of the pipeline, Pusher will produce serving 
      model under model_serve_dir.
    type: string
    value: "{{ PIPELINE_ROOT }}/serving_model"
  DATAFLOW_BEAM_PIPELINE_ARGS:
    description: |
      Settings for beam pipeline. Needed for bigquery. 
    type: object
    value:
      project: "{{ GOOGLE_CLOUD_PROJECT }}"
      runner: "DataflowRunner"
      temp_location: "{{ GCS_BUCKET_NAME }}/tmp"
      region: "{{ GOOGLE_CLOUD_REGION }}"
      disk_size_gb: 50
      machine_type: e2-standard-8
  GCP_AI_PLATFORM_TRAINING_ARGS:
    description: GCP AI platform settings. For GPU usage, see guides at https://cloud.google.com/ai-platform/training/docs/using-gpus
    type: object
    value:
      project: "{{ GOOGLE_CLOUD_PROJECT }}"
      region:  us-east4
      scaleTier: CUSTOM
      masterType: n1-highmem-16
      masterConfig:
        imageUri: "gcr.io/{{ GOOGLE_CLOUD_PROJECT }}/tfx-pipeline"
        acceleratorConfig:
          count: 1
          type: NVIDIA_TESLA_P4
  KUBEFLOW_RUNNER:
    description: Kubeflow runner .py script to use
    type: string
    value: kubeflow_runner.py
  enable_cache:
    description: Whether or not to enable caching of execution results
    type: boolean
    value: False
  enable_gpc_ai_platform_training:
    description: Whether or not to enable GPU training with GCP AI Platform
    type: boolean
    value: True
  preprocessing_fn:
    description: Import path to the preprocessing function relative to the project directory
    type: string
    value: models.preprocessing.preprocessing_fn
  run_fn:
    description: Import path to the model run function relative to the project directory
    type: string
    value: models.node2vec.model.run_fn
model_configurations:
  query_script_path:
    description: Path to BigQuery script, relative to the project directory
    type: string
    value: data/graph_data.sql
  query_debug_settings:
    description: BigQuery script debug settings, such as limit number of rows of data to return, etc.
    type: string
    value: "LIMIT 10"
  seed:
    description: Starting seed of graph sample genration. Use null to avoid seeding.
    type: integer
    value: null
  num_epochs: 
    description: Number of training epochs
    type: integer
    value: 30
  train_batch_size: 
    description: Batch size of training
    type: integer
    value: 128
  eval_batch_size:
    description: Batch size of evaluation. If null, use training batch size
    type: integer
    value: null
  embed_size: 
    description: Embedding size of the nodes
    type: integer
    value: 32
  num_neg_samples:
    description: Number of negative samples to take for subsample softmax training
    type: integer
    value: 15
  walk_length:
    description: node2vec graph traversal walk length
    type: integer
    value: 30
  train_repetitions:
    description: node2vec graph traversal repetitions for training dataset
    type: integer
    value: 6
  eval_repetitions:
    description: node2vec graph traversal repetitions for evaluation dataset
    type: integer
    value: 1
  p:
    description: node2vec Return parameter
    type: float
    value: 1.0
  q:
    description: node2vec In-Out parameters
    type: float
    value: 1.0
  window_size:
    description: word2vec window size
    type: int
    value: 8