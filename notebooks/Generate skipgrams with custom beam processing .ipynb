{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "lined-clear",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T18:52:51.636046Z",
     "start_time": "2021-04-07T18:52:51.630871Z"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import tempfile\n",
    "from absl import logging\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import apache_beam as beam\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "driven-middle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T18:26:57.710267Z",
     "start_time": "2021-04-07T18:26:57.674274Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_schema': feature {\n",
       "  name: \"s0\"\n",
       "  type: INT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"s1\"\n",
       "  type: INT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "feature {\n",
       "  name: \"s2\"\n",
       "  type: INT\n",
       "  presence {\n",
       "    min_fraction: 1.0\n",
       "  }\n",
       "  shape {\n",
       "  }\n",
       "}\n",
       "}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf.constant([[1,2, 3], [4, 5, 6], [1, 2, 3], [6, 7, 8], [2, 3, 5], [3, 5, 7]], dtype=\"int64\")\n",
    "dataset = tf.data.Dataset.from_tensor_slices({f\"s{i}\": features[:, i] for i in range(features.shape[1])})\n",
    "dataset_schema = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec({\n",
    "        f's{i}': tf.io.FixedLenFeature([], tf.int64)\\\n",
    "        for i in range(features.shape[1])\n",
    "    }))\n",
    "dataset_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "overall-evaluation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T18:31:36.513260Z",
     "start_time": "2021-04-07T18:31:36.446105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([3, 2, 2, 1, 1, 3, 4, 5, 6, 4, 6, 5, 2, 1, 1, 2, 3, 3, 8, 8, 7, 6,\n",
       "        7, 6, 2, 3, 5, 5, 2, 3])>,\n",
       " 'context': <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([1, 1, 3, 2, 3, 2, 6, 6, 5, 5, 4, 4, 1, 2, 3, 3, 1, 2, 7, 6, 6, 8,\n",
       "        8, 7, 5, 5, 2, 3, 3, 2])>,\n",
       " 'label': <tf.Tensor: shape=(30,), dtype=int64, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1])>}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_preproc_func(vocabulary_size, window_size, negative_samples):\n",
    "    \"\"\"Returns a preprocessing_fn to make skipgrams given the parameters.\"\"\"\n",
    "    def _make_skipgrams(s):\n",
    "        \"\"\"Numpy function to make skipgrams.\"\"\"\n",
    "        pairs, labels = skipgrams(\n",
    "                s, vocabulary_size=vocabulary_size, window_size=window_size, negative_samples=0,\n",
    "            )\n",
    "        samples = np.concatenate([np.asarray(pairs), np.asarray(labels)[:, None]], axis=1)\n",
    "        return samples\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_make_skipgrams(s):\n",
    "        \"\"\"tf nump / function wrapper.\"\"\"\n",
    "        y = tf.numpy_function(_make_skipgrams, [s], tf.int64)\n",
    "        return y\n",
    "    \n",
    "    def _fn(inputs):\n",
    "        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "        S = tf.stack(list(inputs.values()), axis=1) # tf tensor\n",
    "        \n",
    "        out = tf.map_fn(tf_make_skipgrams, S)\n",
    "        out = tf.reshape(out, (-1, 3))\n",
    "        \n",
    "        output = {}\n",
    "        output[\"target\"] = out[:, 0]\n",
    "        output[\"context\"] = out[:, 1]\n",
    "        output[\"label\"] = out[:, 2]\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return _fn\n",
    "\n",
    "preprocessing_fn = make_preproc_func(12, 2, 0.2)\n",
    "output = preprocessing_fn(list(dataset.batch(5).as_numpy_iterator())[0])\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-female",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def make_preproc_func(vocabulary_size, window_size, negative_samples):\n",
    "    def _make_skipgrams(s):\n",
    "        pairs, labels = skipgrams(\n",
    "                s, vocabulary_size=vocabulary_size, window_size=window_size, negative_samples=0,\n",
    "            )\n",
    "        samples = np.concatenate([np.asarray(pairs), np.asarray(labels)[:, None]], axis=1)\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _fn(inputs):\n",
    "        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "        S = np.stack(list(inputs.values()), axis=1) # tf tensor\n",
    "        print(inputs)\n",
    "        \n",
    "        out = np.apply_along_axis(_make_skipgrams, axis=1, arr=S).reshape((-1, 3))\n",
    "        \n",
    "        output = {}\n",
    "        output[\"target\"] = out[:, 0]\n",
    "        output[\"context\"] = out[:, 1]\n",
    "        output[\"label\"] = out[:, 2]\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return _fn\n",
    "\n",
    "preprocessing_fn = make_preproc_func(12, 2, 0.2)\n",
    "output = preprocessing_fn(list(dataset.batch(5).as_numpy_iterator())[0])\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "residential-incentive",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T18:32:15.256226Z",
     "start_time": "2021-04-07T18:32:14.328957Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpzlg47iob/tftransform_tmp/4664b7dad2bf4bfd85e19aaf27b3e058/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmpzlg47iob/tftransform_tmp/4664b7dad2bf4bfd85e19aaf27b3e058/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-e15132b9-534a-4a20-9783-e37baaf3795f.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "<TensorSliceDataset shapes: {s0: (), s1: (), s2: ()}, types: {s0: tf.int64, s1: tf.int64, s2: tf.int64}>\n",
      "\n",
      "Transformed data:\n",
      "[{'context': 1, 'label': 1, 'target': 3},\n",
      " {'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 1, 'label': 1, 'target': 2},\n",
      " {'context': 2, 'label': 1, 'target': 1},\n",
      " {'context': 3, 'label': 1, 'target': 1},\n",
      " {'context': 5, 'label': 1, 'target': 6},\n",
      " {'context': 4, 'label': 1, 'target': 5},\n",
      " {'context': 5, 'label': 1, 'target': 4},\n",
      " {'context': 6, 'label': 1, 'target': 5},\n",
      " {'context': 4, 'label': 1, 'target': 6},\n",
      " {'context': 6, 'label': 1, 'target': 4},\n",
      " {'context': 2, 'label': 1, 'target': 1},\n",
      " {'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 1, 'label': 1, 'target': 2},\n",
      " {'context': 1, 'label': 1, 'target': 3},\n",
      " {'context': 3, 'label': 1, 'target': 1},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 6, 'label': 1, 'target': 8},\n",
      " {'context': 8, 'label': 1, 'target': 7},\n",
      " {'context': 8, 'label': 1, 'target': 6},\n",
      " {'context': 7, 'label': 1, 'target': 6},\n",
      " {'context': 7, 'label': 1, 'target': 8},\n",
      " {'context': 6, 'label': 1, 'target': 7},\n",
      " {'context': 2, 'label': 1, 'target': 5},\n",
      " {'context': 5, 'label': 1, 'target': 3},\n",
      " {'context': 3, 'label': 1, 'target': 5},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 5, 'label': 1, 'target': 2},\n",
      " {'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 3, 'label': 1, 'target': 7},\n",
      " {'context': 5, 'label': 1, 'target': 7},\n",
      " {'context': 7, 'label': 1, 'target': 3},\n",
      " {'context': 7, 'label': 1, 'target': 5},\n",
      " {'context': 5, 'label': 1, 'target': 3},\n",
      " {'context': 3, 'label': 1, 'target': 5}]\n"
     ]
    }
   ],
   "source": [
    "# Run the beam pipeline\n",
    "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "    transformed_dataset, transform_fn = (  # pylint: disable=unused-variable\n",
    "        (dataset.as_numpy_iterator(), dataset_schema) | tft_beam.AnalyzeAndTransformDataset(\n",
    "            preprocessing_fn))\n",
    "\n",
    "transformed_data, transformed_metadata = transformed_dataset  # pylint: disable=unused-variable\n",
    "\n",
    "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(dataset)))\n",
    "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "sublime-fantasy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:36:05.073481Z",
     "start_time": "2021-04-07T21:36:03.690743Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3)\n",
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmp3noze5na/tftransform_tmp/3765f6302a544f5289b5554e75fb8cf5/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /tmp/tmp3noze5na/tftransform_tmp/3765f6302a544f5289b5554e75fb8cf5/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-e15132b9-534a-4a20-9783-e37baaf3795f.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py', '-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-e15132b9-534a-4a20-9783-e37baaf3795f.json']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw data:\n",
      "<TensorSliceDataset shapes: {s0: (), s1: (), s2: ()}, types: {s0: tf.int64, s1: tf.int64, s2: tf.int64}>\n",
      "\n",
      "Transformed data:\n",
      "[{'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 3, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 2},\n",
      " {'context': 1, 'label': 1, 'target': 3},\n",
      " {'context': 2, 'label': 1, 'target': 1},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 6, 'label': 1, 'target': 5},\n",
      " {'context': 6, 'label': 1, 'target': 4},\n",
      " {'context': 4, 'label': 1, 'target': 5},\n",
      " {'context': 4, 'label': 1, 'target': 6},\n",
      " {'context': 5, 'label': 1, 'target': 4},\n",
      " {'context': 5, 'label': 1, 'target': 6},\n",
      " {'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 3, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 2},\n",
      " {'context': 1, 'label': 1, 'target': 3},\n",
      " {'context': 2, 'label': 1, 'target': 1},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 8, 'label': 1, 'target': 7},\n",
      " {'context': 8, 'label': 1, 'target': 6},\n",
      " {'context': 6, 'label': 1, 'target': 7},\n",
      " {'context': 6, 'label': 1, 'target': 8},\n",
      " {'context': 7, 'label': 1, 'target': 6},\n",
      " {'context': 7, 'label': 1, 'target': 8},\n",
      " {'context': 5, 'label': 1, 'target': 3},\n",
      " {'context': 5, 'label': 1, 'target': 2},\n",
      " {'context': 2, 'label': 1, 'target': 3},\n",
      " {'context': 2, 'label': 1, 'target': 5},\n",
      " {'context': 3, 'label': 1, 'target': 2},\n",
      " {'context': 3, 'label': 1, 'target': 5},\n",
      " {'context': 7, 'label': 1, 'target': 5},\n",
      " {'context': 7, 'label': 1, 'target': 3},\n",
      " {'context': 3, 'label': 1, 'target': 5},\n",
      " {'context': 3, 'label': 1, 'target': 7},\n",
      " {'context': 5, 'label': 1, 'target': 3},\n",
      " {'context': 5, 'label': 1, 'target': 7},\n",
      " {'context': 1, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 1},\n",
      " {'context': 1, 'label': 1, 'target': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Putting everything together\n",
    "def make_preproc_func(vocabulary_size, window_size, negative_samples):\n",
    "    \"\"\"Returns a preprocessing_fn to make skipgrams given the parameters.\"\"\"\n",
    "    def _make_skipgrams(s):\n",
    "        \"\"\"Numpy function to make skipgrams.\"\"\"\n",
    "        pairs, labels = skipgrams(\n",
    "                s, vocabulary_size=100, window_size=window_size, \n",
    "                negative_samples=negative_samples, seed=42,\n",
    "            )\n",
    "        samples = np.concatenate([np.asarray(pairs), np.asarray(labels)[:, None]], axis=1)\n",
    "        return samples\n",
    "    \n",
    "    @tf.function\n",
    "    def _tf_make_skipgrams(s):\n",
    "        \"\"\"tf nump / function wrapper.\"\"\"\n",
    "        y = tf.numpy_function(_make_skipgrams, [s], tf.int64)\n",
    "        y.set_shape([None, 3])\n",
    "        return y\n",
    "    \n",
    "    def _fn(inputs):\n",
    "        \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "        S = tf.stack(list(inputs.values()), axis=1) # tf tensor\n",
    "        \n",
    "        if False: # taking care of variable size tensors\n",
    "            out = tf.map_fn(_tf_make_skipgrams, S, \n",
    "                            fn_output_signature=tf.RaggedTensorSpec(shape=[None, 3], ragged_rank=0, \n",
    "                                                                    dtype=tf.int64))\n",
    "            \n",
    "            out = out.to_tensor(default_value=-1)\n",
    "            out = tf.reshape(out, (-1, 3))\n",
    "            index = tf.reduce_all(tf.greater(out, -1), axis=1)\n",
    "            out = tf.boolean_mask(out, index, axis=0)\n",
    "        else:\n",
    "            out = tf.map_fn(_tf_make_skipgrams, S)\n",
    "            out = tf.reshape(out, (-1, 3))\n",
    "        \n",
    "        output = {}\n",
    "        output[\"target\"] = out[:, 0]\n",
    "        output[\"context\"] = out[:, 1]\n",
    "        output[\"label\"] = out[:, 2]\n",
    "\n",
    "        return output\n",
    "    \n",
    "    return _fn\n",
    "\n",
    "\n",
    "def generate_skipgrams(features, vocabulary_size=10, window_size=2, negative_samples=0., feature_names=None, save_path=\"temp\"):\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"f{i}\" for i in range(features.shape[1])]\n",
    "    assert len(feature_names) == features.shape[1]\n",
    "    \n",
    "    # Convert to list of dict dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices({f\"s{i}\": features[:, i] for i in range(features.shape[1])})\n",
    "    dataset_schema = dataset_metadata.DatasetMetadata(\n",
    "        schema_utils.schema_from_feature_spec({\n",
    "            f's{i}': tf.io.FixedLenFeature([], tf.int64)\\\n",
    "            for i in range(features.shape[1])\n",
    "        }))\n",
    "    \n",
    "    # Make the preprocessing_fn\n",
    "    preprocessing_fn = make_preproc_func(vocabulary_size, window_size, negative_samples)\n",
    "    \n",
    "    # Run the beam pipeline\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "        transformed_dataset, transform_fn = (  # pylint: disable=unused-variable\n",
    "            (dataset.as_numpy_iterator(), dataset_schema) \n",
    "            | \"Make Skipgrams \" >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "        )\n",
    "\n",
    "    # pylint: disable=unused-variable\n",
    "    transformed_data, transformed_metadata = transformed_dataset  \n",
    "    saved_results = (transformed_data\n",
    "        | \"Write to TFRecord\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "            file_path_prefix=save_path, file_name_suffix=\".tfrecords\",\n",
    "            coder=tft.coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "        )\n",
    "    print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(dataset)))\n",
    "    print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))\n",
    "    # Return the list of paths of tfrecords\n",
    "    num_rows_saved = len(transformed_data)\n",
    "    \n",
    "    return saved_results, num_rows_saved\n",
    "    \n",
    "\n",
    "features = tf.constant([[1,2, 3], [4, 5, 6], [1, 2, 3], [6, 7, 8], [2, 3, 5], [3, 5, 7], [1, 1, 1]], dtype=\"int64\")\n",
    "\n",
    "saved_results, n = generate_skipgrams(features)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "realistic-nothing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:36:08.361889Z",
     "start_time": "2021-04-07T21:36:08.356624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cooked-cycling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T21:21:29.838376Z",
     "start_time": "2021-04-07T21:21:29.823130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-vessel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "first-writer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T19:09:36.328643Z",
     "start_time": "2021-04-07T19:09:36.225446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([2, 2, 6, 4, 2, 3, 8])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([5, 3, 8, 5, 3, 1, 7])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([5, 7, 5, 5, 1, 1, 6])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([6, 8, 3, 3, 3, 3, 4])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([3, 2, 3, 7, 2, 6, 5])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([5, 3, 5, 6, 1, 5, 2])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([5, 3, 1, 3, 4, 3, 5])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([4, 2, 2, 7, 6, 1, 7])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([3, 3, 7, 2, 8, 6, 1])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([2, 2, 5, 1, 6, 7, 2])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([7, 3, 1, 2, 2, 2, 7])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([3, 1, 3, 5, 3, 3, 6])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([5, 8, 3, 7, 8, 3, 6])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([3, 7, 1, 3, 6, 2, 4])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 7, 4, 5, 1, 3, 6])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([2, 5, 6, 6, 3, 5, 7])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([6, 3, 2, 2, 3, 6, 3])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([8, 2, 1, 3, 7, 5, 5])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(7,), dtype=int64, numpy=array([7, 4, 3, 5, 1, 2, 5])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([8, 5, 2, 2, 2, 1, 4])>), <tf.Tensor: shape=(7,), dtype=int64, numpy=array([1, 1, 1, 1, 1, 1, 1])>)\n",
      "((<tf.Tensor: shape=(2,), dtype=int64, numpy=array([5, 5])>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 7])>), <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1])>)\n"
     ]
    }
   ],
   "source": [
    "# Read the skipgrams\n",
    "# Read\n",
    "\n",
    "def tfrecord2dataset(file_pattern, feature_spec, label_key, batch_size=5, \n",
    "                       num_epochs=2):\n",
    "    \"\"\"Returns:\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=file_pattern,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=num_epochs,\n",
    "      features=feature_spec,\n",
    "      label_key=label_key)\n",
    "    #dataset = tf.data.TFRecord()\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def skipgram_loader_map_fn(x, y):\n",
    "    return (x[\"target\"], x[\"context\"]), y\n",
    "\n",
    "\n",
    "feature_spec = {\n",
    "    \"target\":    tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "    \"context\":    tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "    \"label\":    tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "}    \n",
    "loaded_dataset = tfrecord2dataset(saved_results, \n",
    "                                    feature_spec, \n",
    "                                    label_key=\"label\",\n",
    "                                    batch_size=7\n",
    "                                    ).map(skipgram_loader_map_fn)\n",
    "for i in loaded_dataset:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-friend",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
